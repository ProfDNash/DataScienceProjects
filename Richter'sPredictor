import numpy as np
import matplotlib.pyplot as plt
import scipy.optimize as opt
from scipy.io import loadmat
import pandas as pd
import itertools
from sklearn.svm import SVC
import random

#### Richter's Predictor: Stacked Modeling ####

##Convert Categorical Data to Numerical Data##
def convertData(dX):
    m,n = dX.shape;
    X = np.zeros((m, n-20)) ##convert 'has_secondary_use' 'superstructure' variables to one column each
    for i in range(m):
        for j in [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]: ##convert 16, 17, 18 to 26, 27, 28
            if j == 15: ##the first 'superstructure' var
                for k in range(11):
                    X[i,j] = (2**k)*dX[varnames[j+k]][i] ##use binary to separate multiple selections
            elif j == 16:  ##legal_ownership_status##
                X[i,j] = categorical_vars[varnames[j+10]][dX[varnames[j+10]][i]]
            elif j == 17:
                X[i,j] = dX[varnames[j+10]][i]
            elif j == 18:  ##'has_secondary_use' general bool
                if dX[varnames[j+10]][i] ==0:
                    X[i,j] = 0
                else:
                    for k in range(10):
                        if dX[varnames[j+10+k+1]][i]==1:
                            X[i,j] = k+1
                            break
            elif varnames[j] in categorical_vars:
                X[i,j] = categorical_vars[varnames[j]][dX[varnames[j]][i]]
            else:
                X[i,j] = dX[varnames[j]][i]
    return X
    
    ##Neural Network cost function with regularization##
def nnCost(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda):  
    ##reshape Theta1 and Theta2 from nn_params
    Theta1=np.reshape(nn_params[0:hidden_layer_size * (input_layer_size + 1)], (hidden_layer_size, input_layer_size + 1))
    Theta2=np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):], (num_labels, hidden_layer_size + 1))
    ##create vectorized version of y
    m=X.shape[0]
    yVec=np.zeros((m,num_labels))
    for i in range(m):
        yVec[i,np.int(y[i].item(0))-1] = 1  ##label 1 corr. to position 0, etc.
    
    a1=np.append(np.matrix(np.ones(m)).transpose(),X,1) ##add a column of ones
    a2=sigmoid(np.matmul(a1,Theta1.transpose()))
    a2=np.append(np.matrix(np.ones(a2.shape[0])).transpose(),a2,1) ##add a column of ones
    a3=sigmoid(np.matmul(a2,Theta2.transpose()))
    J=np.sum(np.multiply(-yVec,np.log(a3)) - np.multiply((1-yVec),np.log(1-a3)), axis=None)/m
    ##add regularization
    Theta1_reg = np.zeros(Theta1.shape)
    Theta1_reg[:,1:] = Theta1[:,1:]
    Theta2_reg = np.zeros(Theta2.shape)
    Theta2_reg[:,1:] = Theta2[:,1:]
    J=J+lmbda*np.sum(np.power(Theta1_reg,2))/(2*m) ##Theta1 regularization
    J=J+lmbda*np.sum(np.power(Theta2_reg,2))/(2*m) ##Theta1 regularization
    return J

##Neural Network gradient with regularization##
def nnGrad(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda):  
    ##reshape Theta1 and Theta2 from nn_params
    Theta1=np.reshape(nn_params[0:hidden_layer_size * (input_layer_size + 1)], (hidden_layer_size, input_layer_size + 1))
    Theta2=np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):], (num_labels, hidden_layer_size + 1))
    ##create vectorized version of y
    m=X.shape[0]
    yVec=np.zeros((m,num_labels))
    for i in range(m):
        yVec[i,np.int(y[i].item(0))-1] = 1
    
    a1=np.append(np.matrix(np.ones(m)).transpose(),X,1) ##add a column of ones
    a2=sigmoid(np.matmul(a1,Theta1.transpose()))
    a2=np.append(np.matrix(np.ones(a2.shape[0])).transpose(),a2,1) ##add a column of ones
    a3=sigmoid(np.matmul(a2,Theta2.transpose()))

    delta3 = a3 - yVec
    delta2 = np.multiply(np.dot(delta3,Theta2),np.multiply(a2,(1-a2)))
    Delta1 = np.matmul(delta2[:,1:].transpose(),a1)
    Delta2 = np.matmul(delta3.transpose(),a2)
    Theta1_grad = Delta1/m
    Theta2_grad = Delta2/m
    ###add regularization
    Theta1_reg = np.zeros(Theta1.shape)
    Theta1_reg[:,1:] = Theta1[:,1:]
    Theta2_reg = np.zeros(Theta2.shape)
    Theta2_reg[:,1:] = Theta2[:,1:]
    Theta1_grad = Theta1_grad + (lmbda/m)*Theta1_reg
    Theta2_grad = Theta2_grad + (lmbda/m)*Theta2_reg
    grad = np.append(np.array(Theta1_grad),np.array(Theta2_grad))
    
    return grad

##Vectorized Sigmoid Function##
def sigmoid(z):
    return 1 / (1 + np.exp(- z))

##Randomly Initialize Weights for a given layer with L_in input connections and L_out output ones##
def randInitWeights(L_in, L_out):
    epsilon_init=0.12
    W = np.random.rand(L_out,L_in+1)*2*epsilon_init - epsilon_init
    return W

##Feature Normalization##
def featureNormalize(X):
    mu = np.mean(X,axis=0)
    std= np.std(X,axis=0)
    X_norm=(X-mu)/std
    return X_norm, mu, std

## use a given value of theta to predict the label on each image##
def predict(theta1, theta2, X):
    m = X.shape[0]
    a1=np.append(np.matrix(np.ones(m)).transpose(),X,1) ##add a column of ones
    a2=sigmoid(np.matmul(a1,theta1.transpose()))
    a2=np.append(np.matrix(np.ones(a2.shape[0])).transpose(),a2,1) ##add a column of ones
    a3=sigmoid(np.matmul(a2,theta2.transpose()))
    p = np.amax(a3,axis=1) ##maximum probability
    g = np.where(a3==p)  ##location of max, i.e. label-1, so add 1 to compensate
    return g, p
    
##First read the data in and convert it##
dX = pd.read_csv('train_values.csv')
dY = pd.read_csv('train_labels.csv')
dT = pd.read_csv('test_values.csv')
categorical_vars = dict()
categorical_vars['land_surface_condition'] = dict(n = 1, o = 2, t = 3)
categorical_vars['foundation_type'] = dict(h=1, i=2, r=3, u=4, w=5)
categorical_vars['roof_type'] = dict(n=1, q=2, x=3)
categorical_vars['ground_floor_type'] = dict(f=1, m=2, v=3, x=4, z=5)
categorical_vars['other_floor_type'] = dict(j=1, q=2, s=3, x=4)
categorical_vars['position'] = dict(j=1, o=2, s=3, t=4)
categorical_vars['plan_configuration'] = dict(a=1, c=2, d=3, f=4, m=5, n=6, o=7, q=8, s=9, u=10)
categorical_vars['legal_ownership_status'] = dict(a = 1, r = 2, v = 3, w = 4)
varnames = list(dX.columns)
DX = convertData(dX)
DXtest = convertData(dT)
mtest = Xtest.shape[0]
Y = np.zeros((dY.shape[0],1))
for i in range(dY.shape[0]): ##each row corresponds to a building
Y[i,0] = dY['damage_grade'][i]


##Setup Initial Parameters##
X = DX.drop(columns=['Unnamed: 0', '0'])
Xtest = DXtest.drop(columns=['Unnamed: 0', '0'])
input_layer_size=X.shape[1]
hidden_layer_size=200
num_labels=3
lmbda=1
alpha=1
init_Theta1 = randInitWeights(input_layer_size, hidden_layer_size)
init_Theta2 = randInitWeights(hidden_layer_size, num_labels)
##unroll the parameters into a single vector##
init_params = np.append(init_Theta1, init_Theta2)
idx = random.sample(range(1,260601),100000) ##choose 100,000 random rows from the dataset
predictions = dict()

##Implement a two-layer neural network to classify patients##
print('Training Neural Network (lambda = 1)...')
Xrand = np.zeros((100000,18))
for i in range(100000):
    Xrand[i] = X.loc[idx[i]]

min_nn_params=opt.fmin_cg(nnCost, init_params, fprime=nnGrad, args=(input_layer_size, hidden_layer_size, num_labels, Xrand, Y[idx], lmbda), maxiter=10000)

input('\nProgram paused.  Press enter to continue.\n')
##reshape Theta1 and Theta2##
mTheta1=np.reshape(min_nn_params[0:hidden_layer_size * (input_layer_size + 1)], (hidden_layer_size, input_layer_size + 1))
mTheta2=np.reshape(min_nn_params[hidden_layer_size * (input_layer_size + 1):], (num_labels, hidden_layer_size + 1))
pred, prob=predict(mTheta1, mTheta2, X)
print('Prediction accuracy on our training set is', np.mean(pred[1]+1==Y.transpose())*100, '%')

Tpred, Tprob = predict(mTheta1, mTheta2, Xtest)

predictions['reg'] = np.multiply((1-Tpred[1].reshape((mtest,1))),(1-Tprob)) + np.multiply(Tpred[1].reshape((mtest,1)),Tprob)


##Random Forest Classifier##
from sklearn.ensemble import RandomForestClassifier
best = 0
estimators = 0
for i in [10,50,100,200,300]:
    RF = RandomForestClassifier(n_estimators = i)
    print('\nPerforming Random Forest Model Fit with {} estimators ... (this may take awhile)'.format(i))
    RF.fit(X,np.array(Y))
    print('Done!')
    print('Model Score with {} estimators: '.format(i), RF.score(X,np.array(Y)))
    if best<RF.score(X,np.array(Y)):
        best = RF.score(X,np.array(Y))
        estimators = i
##best appears to be when i = 100-200
RF = RandomForestClassifier(n_estimators = 100)
RF.fit(X,np.array(Y))
pred['RF'] = RF.predict(Xtest)


##TRY GRADIENT BOOSTING CLASSIFICATION ##
from sklearn.ensemble import GradientBoostingClassifier
best = 0
estimators = 0
depth = 0
for i in [500, 1000, 2000, 10000]:
    for j in [5,6,7,8,9]:
        gbc = GradientBoostingClassifier(n_estimators = i, max_depth=j)
        print('\nFitting the Gradient Boosting Classifier ({},{})... (this may take awhile)'.format(i,j))
        gbc.fit(X,np.array(Y))
        print('\nDone!')
        score = gbc.score(X,np.array(Y))
        print('\nGBC.score with ({},{}): '.format(i,j), score)
        if best < score:
            best = score
            estimators = i
            depth = j

##best appears to be 10000 estimators with depth 8 -- depth 9 is more accurate, but takes much longer##
gbc = GradientBoostingClassifier(n_estimators = 10000, max_depth=8)
print('\nFitting the Gradient Boosting Classifier... (this may take awhile)')
gbc.fit(X,Y)
print('Done!')
predictions['GBC'] = gbc.predict(Xtest)



##TRY SUPPORT VECTOR MACHINES##
svclassifier = SVC(kernel = 'linear')
print('\nFitting SVM model... (this may take awhile)')
svclassifier.fit(Xrand,np.array(Y[idx]))
print('Done!')
pred['SVC'] = svclassifier.predict(Xtest)
##This takes way too long, even with a subset of the data, and isn't very accurate##



##TRY NAIVE BAYES CLASSIFIER##
from sklearn.naive_bayes import GaussianNB, MultinomialNB
NB = GaussianNB()
MNB = MultinomialNB()
print('\nFitting Naive-Bayes Classifiers... (this may take awhile)')
NB.fit(X,np.array(Y))
MNB.fit(X,np.array(Y))
print('Done!')
print('Gaussian Naive-Bayes: ', NB.score(X,np.array(Y)))
predictions['NB'] = NB.predict(Xtest)
print('Multinomial Naive-Bayes: ', MNB.score(X, np.array(Y)))
predictions['MNB'] = MNB.predict(Xtest)
